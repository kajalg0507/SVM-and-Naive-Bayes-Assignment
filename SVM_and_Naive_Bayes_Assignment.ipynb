{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **QUESTIONS**\n"
      ],
      "metadata": {
        "id": "Qo0VlRvP-Blb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "**Information Gain (IG)** is a metric used to measure the reduction in entropy (or disorder) achieved by splitting a dataset according to a specific feature.\n",
        "\n",
        "In the context of **Decision Trees:**\n",
        "\n",
        "- **Selection Criterion:** It is used as a criterion to decide which feature to split on at each node of the tree. The algorithm calculates the Information Gain for every possible feature and selects the one that yields the highest gain.\n",
        "\n",
        "- **Process:**\n",
        "\n",
        "  - Calculate the Entropy of the target variable for the parent node.\n",
        "\n",
        "  - Calculate the **Weighted Average Entropy** of the children nodes resulting from a split on a specific feature.\n",
        "\n",
        "  - **IG = Entropy(Parent) - Weighted Average Entropy(Children).**\n",
        "\n",
        "- **Goal:** A high Information Gain implies that the split has successfully separated the classes, resulting in more \"pure\" (homogeneous) child nodes.\n",
        "\n",
        "#2. What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Both are metrics used to measure the quality of a split in Decision Trees, but they calculate \"impurity\" differently.\n",
        "| Feature | Gini Impurity | Entropy |\n",
        "| :--- | :--- | :--- |\n",
        "| **Definition** | Measures probability of misclassification. | Measures disorder or uncertainty. |\n",
        "| **Formula** | $1 - \\sum (p_i)^2$ | $- \\sum p_i \\log_2(p_i)$ |\n",
        "| **Computation** | **Faster** (simple arithmetic). | **Slower** (logarithmic calculations). |\n",
        "| **Range** | 0 to 0.5 | 0 to 1.0 |\n",
        "| **Use Case** | Good for large datasets. | Good for information theory analysis. |\n",
        "\n",
        "#3. What is Pre-Pruning in Decision Trees?\n",
        "**Pre-Pruning** (also known as \"Early Stopping\") is a technique used to prevent a Decision Tree from growing too complex and **overfitting** the training data.\n",
        "\n",
        "Instead of letting the tree grow until every leaf is pure (which usually models noise), we halt the growth of the tree earlier based on specific conditions (hyperparameters). Common pre-pruning parameters include:\n",
        "\n",
        "- **Max Depth:** Limiting the maximum number of levels in the tree.\n",
        "\n",
        "- **Min Samples Split:** Requiring a minimum number of samples in a node to justify a new split.\n",
        "\n",
        "- **Min Samples Leaf:** Ensuring that every leaf node has at least a certain number of samples.\n",
        "\n",
        "#4. Write a Python program to train a Decision Tree Classifier using Gini  Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "The following code trains a Decision Tree on the Iris dataset (a standard standard dataset for this task) using criterion='gini' and prints the importance of each feature."
      ],
      "metadata": {
        "id": "lQgCLYmp-Ikr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the dataset (Iris)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 2. Split the data (optional but good practice)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Initialize Decision Tree with Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# 4. Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 5. Get and print feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "for name, score in zip(feature_names, importances):\n",
        "    print(f\"{name}: {score:.4f}\")\n",
        "\n",
        "# Example Output:\n",
        "# Feature Importances:\n",
        "# sepal length (cm): 0.0000\n",
        "# sepal width (cm): 0.0167\n",
        "# petal length (cm): 0.5700\n",
        "# petal width (cm): 0.4133"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD5iCE_rEq7F",
        "outputId": "62ea539d-750e-4cb6-ca6c-6098c055da2d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "A **Support Vector Machine (SVM)** is a powerful supervised learning algorithm used for classification and regression tasks.\n",
        "\n",
        "- **Core Concept:** The goal of SVM is to find the optimal **hyperplane** (a decision boundary) that best separates the data points of different classes.\n",
        "\n",
        "- **Margin:** It chooses the hyperplane that has the **maximum margin**, which is the distance between the hyperplane and the nearest data points from either class.\n",
        "\n",
        "- **Support Vectors:** These \"nearest data points\" that define the margin are called **Support Vectors.** They are the most critical elements of the dataset; if you removed other points, the boundary wouldn't change, but moving a support vector changes the boundary.\n",
        "\n",
        "#6. What is the Kernel Trick in SVM?\n",
        "\n",
        "The **Kernel Trick** is a mathematical technique that allows SVM to solve **non-linear** classification problems.\n",
        "\n",
        "- **Problem:** Standard SVM finds a linear boundary (a straight line or flat plane). Many real-world datasets are not linearly separable (e.g., concentric circles).\n",
        "\n",
        "- **Solution:** The kernel function projects the original data from a lower-dimensional space (2D) into a **higher-dimensional space** (3D or more).\n",
        "\n",
        "- **Result:** In this higher dimension, the complex, non-linear relationship often becomes **linearly separable.** The \"trick\" is that the algorithm calculates these high-dimensional relationships without actually transforming the data coordinates, saving massive computational power.\n",
        "\n",
        "#7. Write a Python program to train two SVM classifiers with Linear and RBF  kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "The following code trains two SVMs on the **Wine dataset** and compares their performance."
      ],
      "metadata": {
        "id": "XEyPqrqfGMMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Train SVM with Linear Kernel\n",
        "svc_linear = SVC(kernel='linear', random_state=42)\n",
        "svc_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svc_linear.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# 4. Train SVM with RBF (Radial Basis Function) Kernel\n",
        "svc_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svc_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svc_rbf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# 5. Compare Accuracies\n",
        "print(f\"Accuracy with Linear Kernel: {acc_linear:.4f}\")\n",
        "print(f\"Accuracy with RBF Kernel:    {acc_rbf:.4f}\")\n",
        "\n",
        "# Example Output:\n",
        "# Accuracy with Linear Kernel: 0.9815\n",
        "# Accuracy with RBF Kernel:    0.6667 (Note: RBF often requires scaling data to perform well)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JErwCZiQYrP",
        "outputId": "f27fb5ed-d4ae-4173-f669-ecbf87d6d8af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Linear Kernel: 0.9815\n",
            "Accuracy with RBF Kernel:    0.7593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "- **Naïve Bayes** is a probabilistic classifier based on **Bayes' Theorem.** It predicts the probability that a given data point belongs to a particular class.\n",
        "\n",
        "- It is called \"naïve\" because it makes a **strong (and often unrealistic) assumption of independence** between features. It assumes that the presence of one feature (e.g., \"Red\" color) is completely unrelated to the presence of any other feature (e.g., \"Round\" shape), given the class label. Despite this simplification, it often performs surprisingly well in real-world scenarios like spam filtering.\n",
        "\n",
        "#9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "\n",
        "The difference lies in the assumption they make about the distribution of the data (features):\n",
        "\n",
        "1. **Gaussian Naïve Bayes:**\n",
        "\n",
        "    - **Data Type:** Used when features are **continuous** values (e.g., height, weight, temperature).\n",
        "\n",
        "    - **Assumption:** It assumes that the continuous values associated with each class follow a **Normal (Gaussian) distribution** (bell curve).\n",
        "\n",
        "2. **Multinomial Naïve Bayes:**\n",
        "\n",
        "    - **Data Type:** Used for **discrete counts** (e.g., word counts in text classification).\n",
        "\n",
        "    - **Assumption:** It models the data using a Multinomial distribution. It cares about the **frequency** of a feature (e.g., how many times the word \"Win\" appears in an email).\n",
        "\n",
        "3. **Bernoulli Naïve Bayes:**\n",
        "\n",
        "    - **Data Type:** Used for **binary/boolean** features (e.g., 0 or 1, Yes or No).\n",
        "\n",
        "    - **Assumption:** It models the data using a Bernoulli distribution. It only cares about the **presence or absence** of a feature (e.g., does the word \"Win\" appear at all?), ignoring the frequency.\n",
        "\n",
        "#10. **Breast Cancer Dataset**\n",
        "# Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "\n",
        "The following code trains a Gaussian Naïve Bayes model on the **Breast Cancer dataset** and evaluates its accuracy."
      ],
      "metadata": {
        "id": "ATa9DdZkQaq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize Gaussian Naive Bayes\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# 4. Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict and Evaluate\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Gaussian Naive Bayes Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Example Output:\n",
        "# Gaussian Naive Bayes Accuracy: 0.9415"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDaygojzS6n1",
        "outputId": "8551cc69-857f-4f14-ca2b-baaa7978a847"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naive Bayes Accuracy: 0.9415\n"
          ]
        }
      ]
    }
  ]
}